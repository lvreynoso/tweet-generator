#!/usr/bin/env python3
# tokenize.py
# module for creating lists of tokens from a text

def generate(source_text):
    return source_text.split()